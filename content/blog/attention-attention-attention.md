---
title: Attention, Attention, Attention
date: 2022-03-12
tags: machine-learning
---

I have wanted to take some time and learn about transformers for a while. They seems to be revolutionizing every major field within AI, so I could no longer afford to stay away.

The other day I decided to start from the beginning and went all in into the seminal paperÂ [Attention is all you need](https://arxiv.org/abs/1706.03762).

The paper is quite interesting, but I failed to grasp the intuition behind the Qs, Ks, and Vs in there. I went looking elsewhere for a good explanation of what each of those was, but most articles just weren't clear enough.

Alas, at last, I found what I was looking for. The following YouTube series of 4 videos goes in depth about how the attention layer works and I recommend it to anyone trying to get started with transformers.

[Youtube Playlist](https://youtu.be/yGTUuEx3GkA)

## Update: 9th March, 2023

I have found another great resource for learning about transformers. This [YouTube video](https://www.youtube.com/watch?v=kCc8FmEb1nY) provides a clear and thorough explanation of the attention mechanism and how it is used in transformer models. I highly recommend it to anyone looking to learn more about this topic.
